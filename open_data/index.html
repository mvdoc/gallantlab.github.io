<!DOCTYPE html>
<html prefix="
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>GallantLab · OpenData </title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700%7CAbril+Fatface">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../rss.xml">
<link rel="canonical" href="https://gallantlab.org/open_data/">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><link rel="stylesheet" type="text/css" href="../assets/css/custom.css">
<meta name="author" content="Jack L. Gallant">
</head>
<body class="">
    <a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

    <div class="hsidebar">
        <div class="container sidebar-sticky">
            <div class="sidebar-about">
              <h1>
                <a href="https://gallantlab.org/">
                      <h1 id="brand"><a href="https://gallantlab.org/" title="GallantLab" rel="home">

        <span id="blog-title">GallantLab</span>
    </a></h1>

                </a>
              </h1>
                <p class="lead">@ UCB</p>

            </div>
                <nav id="menu" role="navigation" class="sidebar-nav"><a class="sidebar-nav-item" href="../">Home</a>
        <a class="sidebar-nav-item" href="../brain_viewers">BrainViewers</a>
        <a class="sidebar-nav-item" href="../papers">Papers</a>
        <a class="sidebar-nav-item" href="../people">People</a>
        <a class="sidebar-nav-item" href="../open_data">OpenData</a>
        <a class="sidebar-nav-item" href="../open_code">OpenCode</a>
        <a class="sidebar-nav-item" href="../learn">Learn</a>
        <a class="sidebar-nav-item" href="../blog">Blog</a>
    
    
    </nav><footer id="footer"><span class="copyright">
              This site was last updated on March 30, 2023 ©          <a href="mailto:gallant@berkeley.edu">Jack L. Gallant</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            </span>
            
            
        </footer>
</div>
    </div>

    <div class="content container" id="content">
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="post-title p-name"><a href="." class="u-url">OpenData</a></h1>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <table><tbody>
<tr>
<td><img alt="/images/papers/Deniz.F.2023.png" src="../images/papers/Deniz.F.2023.png"></td>
<td><p><a class="reference external" href="https://berkeley.app.box.com/v/Deniz-et-al-2019">This 3T fMRI data set consists of recordings made from participants
who were listening to or reading narrative stories.</a>
These data were collected as part of our 2023 study of context
effects on semantic representation
(<a class="reference external" href="https://www.biorxiv.org/content/10.1101/2021.12.15.472839.full.pdf">Semantic representations during language production are affected by
context, Deniz et al., J. Neuroscience, 2023</a>).</p></td>
</tr>
<tr>
<td><img alt="/images/papers/Huth.A.2016.png" src="../images/papers/Huth.A.2016.png"></td>
<td><p><a class="reference external" href="https://berkeley.app.box.com/s/l95gie5xtv56zocsgugmb7fs12nujpog">This 3T fMRI data set consists of recordings made from participants
who were listening to narrative stories, or who were watching a collection
of short video clips.</a>
These data were collected as part of our 2016 study on semantic maps
(<a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309/">Natural speech reveals the semantic maps that tile the
human cerebral cortex, Huth et al., Nature, 2016</a>).
They were also used in our 2021 study of the alignment
of semantic selectivity along the anterior border of visual cortex
(<a class="reference external" href="https://www.nature.com/articles/s41593-021-00921-6">Visual and linguistic semantic representations are aligned at the
border of human visual cortex, Popham et al., Nature Neuroscience, 2021</a>).</p></td>
</tr>
<tr>
<td><img alt="/images/papers/Zhang.T.2021.jpg" src="../images/papers/Zhang.T.2021.jpg"></td>
<td><p><a class="reference external" href="http://crcns.org/data-sets/vc/vim-4/about-vim-4">This 3T fMRI data set consists of recordings made from participants
who were playing the Counterstrike video game.</a>
These data were collected as part of our 2021 study of state spaces
and attention (Voxel-based state space modeling recovers task-related
cognitive states in naturalistic fmri experiments., <a class="reference external" href="https://www.frontiersin.org/articles/10.3389/fnins.2020.565976/full">Zhang et al,
Front. Neuroscience, 2021</a>).</p></td>
</tr>
</tbody></table>
</div>
    
</article>
</div>
            <script src="../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script>
</body>
</html>
