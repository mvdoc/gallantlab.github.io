<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# " lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>BrainViewers | GallantLab@UCBerkeley</title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:700,900" rel="stylesheet">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://gallantlab.org/brain_viewers/">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]--><link rel="stylesheet" type="text/css" href="../assets/css/custom.css">
<meta name="author" content="Jack L. Gallant">
<meta property="og:site_name" content="GallantLab@UCBerkeley">
<meta property="og:title" content="BrainViewers">
<meta property="og:url" content="https://gallantlab.org/brain_viewers/">
<meta property="og:description" content="TEMPORARILY OUT OF SERVICE AS OF 29MAR2023- PLEASE CHECK BACK IN A FEW DAYS
This page collects public brain viewers that you can use to interact
with the data and results from many of our published st">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2023-03-24T23:52:52-07:00">
<meta property="article:tag" content="demos">
<meta property="article:tag" content="interactive">
<meta property="article:tag" content="pycortex">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Header and menu bar -->
<div class="container">
      <header class="blog-header py-3"><div class="row nbb-header align-items-center">
          <div class="col-md-3 col-xs-2 col-sm-2" style="width: auto;">
            <button class="navbar-toggler navbar-light bg-light nbb-navbar-toggler" type="button" data-toggle="collapse" data-target=".bs-nav-collapsible" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse bs-nav-collapsible bootblog4-search-form-holder">
                
            </div>
        </div>
          <div class="col-md-6 col-xs-10 col-sm-10 bootblog4-brand" style="width: auto;">
            <a class="navbar-brand blog-header-logo text-dark" href="../">

            <span id="blog-title">GallantLab@UCBerkeley</span>
        </a>
          </div>
            <div class="col-md-3 justify-content-end align-items-center bs-nav-collapsible collapse flex-collapse bootblog4-right-nav">
            <nav class="navbar navbar-light bg-white"><ul class="navbar-nav bootblog4-right-nav">
<li class="nav-item">
    <a href="index.rst" id="sourcelink" class="nav-link">Source</a>
    </li>


                    
            </ul></nav>
</div>
    </div>
</header><nav class="navbar navbar-expand-md navbar-light bg-white static-top"><div class="collapse navbar-collapse bs-nav-collapsible" id="bs-navbar">
            <ul class="navbar-nav nav-fill d-flex w-100">
<li class="nav-item active">
<a href="." class="nav-link">Home <span class="sr-only">(active)</span></a>
                </li>
<li class="nav-item">
<a href="../brain_viewers" class="nav-link">BrainViewers</a>
                </li>
<li class="nav-item">
<a href="../papers" class="nav-link">Papers</a>
                </li>
<li class="nav-item">
<a href="../people" class="nav-link">People</a>
                </li>
<li class="nav-item">
<a href="../open_data" class="nav-link">OpenData</a>
                </li>
<li class="nav-item">
<a href="../open_code" class="nav-link">OpenCode</a>
                </li>
<li class="nav-item">
<a href="../learn" class="nav-link">Learn</a>
                </li>
<li class="nav-item">
<a href="../blog" class="nav-link">Blog</a>

                
            </li>
</ul>
</div>
<!-- /.navbar-collapse -->
</nav>
</div>

<div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">BrainViewers</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <section id="temporarily-out-of-service-as-of-29mar2023-please-check-back-in-a-few-days"><h2>TEMPORARILY OUT OF SERVICE AS OF 29MAR2023- PLEASE CHECK BACK IN A FEW DAYS</h2>
<p>This page collects public brain viewers that you can use to interact
with the data and results from many of our published studies. To reach
the brain viewer for any topic, just click on the highlighted hyperlink.
Please note that these brain viewers do not run well on cell phones,
you will have the best experience with a computer or a tablet.</p>
<table><tbody>
<tr>
<td><img alt="/images/other/viewer.Deniz.F.2019.png" src="../images/other/viewer.Deniz.F.2019.png"></td>
<td><p><a class="reference external" href="https://169.229.219.171">The representation of semantic information across human cerebral
cortex during listening versus reading is invariant to stimulus
modality
(Deniz et al., J. Neuroscience, 2019</a>).
In this experiment, people listened to and read stories from the
Moth Radio Hour while brain activity was recorded. Voxelwise
modeling was used to determine how each individual brain location
responded to semantic concepts in the stories during listening and
reading, separately. The interactive brain viewer shows how these
concepts are mapped across the cortical surface for both modalities
(listening and reading). The colors on the cortical map indicate the
semantic concepts that will elicit brain activity at that location
during listening and reading.</p></td>
</tr>
<tr>
<td><img alt="/images/other/viewer.Lescroart.M.2019.jpg" src="../images/other/viewer.Lescroart.M.2019.jpg"></td>
<td><p><a class="reference external" href="https://www.gallantlab.org/brainviewer/Lescroart2018/">Human scene-selective areas represent the 3D configuration of
surfaces
(Lescroart et al., Neuron, 2019</a>).
In this experiment people viewed rendered animations depicting
objects placed in scenes. The MRI data were analyzed by
voxelwise modeling to recover the cortical representation of
low-level features and 3D structure. This demo shows how surface
position, distance and orientation are mapped across the
cortical surface.</p></td>
</tr>
<tr>
<td><img alt="/images/other/viewer.Huth.A.2016.jpg" src="../images/other/viewer.Huth.A.2016.jpg"></td>
<td><p><a class="reference external" href="https://www.gallantlab.org/Huth2016/">Natural speech reveals the semantic maps that tile human
cerebral cortex
(Huth et al., Nature, 2016</a>).
In this experiment people passively listened to stories from the
Moth Radio Hour while brain activity was recorded. Voxelwise
modeling was used to determine how each individual brain location
responded to 985 distinct semantic concepts in the stories. The
demo shows how these concepts are mapped across the cortical surface.
The colors on the cortical map show indicate the semantic concepts
that will elicit brain activity at that location. The word cloud at
right shows words that the model predicts would evoke the largest
brain response at the indicated location. Follow the tutorial at
upper right to find out more about this tool.</p></td>
</tr>
<tr>
<td><img alt="/images/other/viewer.Cukur.T.2013.jpg" src="../images/other/viewer.Cukur.T.2013.jpg"></td>
<td><p><a class="reference external" href="https://gallantlab.org/brainviewer/cukuretal2013/">Attention during natural vision warps semantic representations
across the human brain
(Cukur et al., Nature Neuroscience, 2013</a>).
In this experiment people passively watched movies while monitoring
for the presence of either “humans” or “vehicles”, and in a neutral
condition. Voxelwise modeling was used to determine how each brain
location responded to 985 distinct categories of objects and actions
in the movies, and how these responses were modulated by attention.
This brain viewer allows you to view data collected under the three
different conditions (left click “Passive Viewing”, “Attending to
Humans” or “Attending to Vehicles”). By selecting single brain
locations (left click on the brain) or single categories (left
click on the WordNet tree), you can see how tuning changes under
different states of attention.</p></td>
</tr>
<tr>
<td><img alt="/images/other/viewer.Huth.A.2012.jpg" src="../images/other/viewer.Huth.A.2012.jpg"></td>
<td><p><a class="reference external" href="https://gallantlab.org/brainviewer/huthetal2012/">A continuous semantic space describes the representation of
thousands of object and action categories across the human brain
(Huth et al., Neuron, 2012</a>).
In this experiment people passively watched movies while brain
activity was recorded. Voxelwise modeling was used to determine
how each brain location responded to 1785 distinct categories
of objects and actions in the movies. The demo shows how these
categories are mapped across the cortical surface. On the left
is the brain of one person, and on the right is the WordNet tree
defining the various categories. The colors painted on the brain
indicate the category selectivity of each location, using
the colors shown on the tree at right. To move the brain, left
click on the brain and move the mouse. To inflate and flatten the
brain left click the buttons at bottom, or use the slider. To see
which categories activate some specific point in the brain, left
click on the brain. This will change the WordNet tree at right so
that it shows categories that activate (red) or suppress (blue)
activity in that voxel. (To return the WordNet tree to the original
colors left click the “Show Semantic Space” button.) To see how some
specific category is represented on the cortical surface, left click
a category in the WordNet tree. This will change the brain so that
it shows locations that are activated (red) or suppressed (blue).</p></td>
</tr>
</tbody></table></section>
</div>
    

</article><!--End of body content--><footer id="footer">
            This site was last updated on March 30, 2023 ©          <a href="mailto:gallant@berkeley.edu">Jack L. Gallant</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
